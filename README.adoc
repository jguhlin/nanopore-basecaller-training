// README

Nanopore Basecaller Training
============================

Several folks are interested, trying to centralize the info.

:toc:
:toc-placement: preamble
:toclevels: 1
:showtitle:

// Setting up conda environment

== Getting Started

=== Install Bonito

[source,shell]
----
conda update -n base -c defaults conda
conda create -n bonito python=3.8 pip 
conda activate --stack bonito
pip install --extra-index-url https://download.pytorch.org/whl/cu116 ont-bonito
pip install -r bonito/requirements.txt
----

Change cu116 to your CUDA version (run nvidia-smi to find it, it'll be on the top right)

=== Download Models
[source,shell]
----
bonito download --models --show
bonito download --models --all
----

You can cancel it at the training data (I think?). My gpugpu machine has very little hard drive space so I did.

=== Create a minimap2 index
[source,shell]
----
minimap2 -d index.mmi assembly.fasta
----

=== Basecall with --save-ctc
[source,shell]
----
bonito basecaller dna_r9.4.1_e8_sup@v3.3 ~/stonefly/all_fast5/ --batchsize 384 --reference index.mmi --save-ctc --recursive --device "cuda:0" --alignment-threads 16 | samtools view -S -b - > basecalls.bam
----

NOTE: It has to be in the order as above, or the ctc will not save! basecaller model filepath THEN options.

Default batch size is 32 (I'm 90% certain). Best to try and increase it. When it crashes from memory, best to killall bonito from another shell. I can get batchsize of 384 and it cuts a little over half an hour off on my dataset (51Gb of fast5 files). And we can work with that number on the following steps.
